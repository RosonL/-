
02_实时数仓串讲
2023年8月10日 下午12:04
36 分 19秒

点击“编辑”，添加会议纪要
说在一年前我们公司开始做实时数仓项目，这个主要是领导层还有运营部门希望看到这个指标结果能够更加的及时，能够更加及时，所以我们准备做一个深入数仓，那当时我也是，你，你是不是也一起过去做了，对吧？那在做实时数仓之前，前期做了一些准备工作，当然这个因为有搭建离线的一些相关经验，也可以参考稍微快一点。
而首先第一我们还是先进行了一个数据调研，但是业务上我们这个时候比较熟悉，比较了解了，没花多少时间，主要就是对需求把它确认清楚。那我们整个实时数仓最终的产出是希望有一个实时大屏，还有一些实时的报表这一些，那么而这个结果的更新，我们是希望能做到分钟级或者秒级，就最低要求分钟级，咳，那首期我们可能就做了十来个核心指标，先把整个电路打通。那接下来其他的像其他准备工作，还有像明确数据域，还有构建业务总线矩阵。注意如果，我先不展开讲这个东西了，我直接跳过了。之后是我们对整个项目架构的设考虑。
首先是这个采集，我们这边做了一些改变，原先离线的时候我们是 from 采集日志，在经过 Kafka 推到Kafka，那对于 MySQL 业务数据我们是用了 data 叉离线批量同步去处理的，那这边要做实施的话，我们是改造了一下业务数据同步链路，这一边我们增加了一个 method 来实时的增量同步，那这样的话同时对离线那边也做了一些修改。

离线的话就是既用 data 叉，又用 mens wear 好 data 差就是小数据量，这些表的每日全量 method 就是一些，大一点的就是实时增量同步，那我们实时数仓里面就主要走的就 mess one 这条链路，也是将数据发送到推到 Kafka 这边去，那这样的话我们两个数据来源就都进入Kafka。
那另外就是我们对整个数仓我们需要做分层，因为要解耦要去做一个解耦，避免重复计算，那这个地方我们存储主要考虑的还是主流的使用消息队列来存储，那这边就选择的是卡夫卡，那其实刚才进过来的这些数据就是作为 ODS 层存在数据贴源层，我们这边主要是保持数据原貌，不做什么修改。在之后我们会去考虑整个数仓的计算引擎使用什么。
我们主要对比的 Flink 还有 Spark sdreaming，我们对比了一下，就 Flink 它不是有计算，而 Spark streaming 它是一个还是一个 VP 次的这么一种处理的计算模型。其次 Flink 它支持这种流式的SQL，也就是说流处理它也可以写 SQL 的方式来开发，而 Spark streaming 并不支持这么这么一种用法。
还有就是关于数据一致性，也就是说它内部的这个数据质量保证的话，也就是有一个 check point 机制。 Flink 这边用的是一种异步分界性快照的算法，更轻量，更高效，而且还能保证一致性。而 Spark 这边它主要这个缺口 point 就是持久化一些元数据，没有什么特别多的东西，就功能点比较弱一点。
还有就是比如说对窗口的使用上面， Flink 它窗口类型多，而且比较灵活。 Spark 追敏只有基于时间的，而且参数配置还有一些限制，必须是批次整数位。还有就是对于乱序迟到数据的处理， Flink 它有 what mark 这么一个机制，那么 Spark 实锤敏并没有这个概念，需要自己去处理。等等。那这一些主要就是我们选择 Flink 的原因。其次 Flink 在国内也是越来越火，越来越多的人在用，所以我们基本上就是也没怎么犹豫，就选择了 Flink 作为我们项目的计算引擎。
好，那么架构整体选型完之后，就是搭建整个数仓的分层去构建出来。那刚才讲了 ODS 层就是在 Kafka 保持原貌，那在这里就是我们存的方式是日志，一个 topic 业务，一个 topic 业务数据是混在一块儿的。在之后就是我们 d 到 d 层跟 DIM 层的设计了，那在这边我们主要去进行一个维度建模，需要去构建我们的事实表，还需要去构建我们的维度表。
那先介绍一下我们 d 到 d 层事实表的构建，那其实在这里我们只构建了一种事实表，就是事物型事实表，这个事务型事实表就是对应最原子的业务过程，一个业务过程会对应一个事务型事实表，比如说一个下单，比如说支付，比如说加购等等。那构建的过程，我们首先第一步选择业务过程，我们会筛选相关的业务表，比如说我构建订单事实表，我就会去筛选出订单表、详情表、状态表、订单活动详情表、订单优惠券详情表等等相关的表整合起来。
另外一个就是生命力度还是一样，在这边考虑最细粒度，因为真要分析后续分期更灵活。第三确定维度，主要就是确定我们去怎么描述这个业务过程的一些客观环境信息，这个在业务总线矩阵已经考虑清楚了，我们这边确定了下有什么时间、地区、商品、用户、活动、优惠券、渠道等等这一些确定事实。第四个确定事实这边主要就是考虑它的度量值，就是一些可以量化的，可以累加，可以统计的一些像次数、件数这些等等，也是业务总线矩阵考虑过。嗯，之所以不做周期型事实表，是因为既然做到实时的，我们也没必要说一个周期去做一个快照，没有必要了，因为它实时在变。那至于累积型，我们是考虑到在流式场景下多流关联能不做就不做，因为多流关联处理起来还是比较复杂，不管是状态的大小还是结果更新的问题，处理起来都麻偏麻烦一点。与其如此，我们就选择不去做了。但是自从我们构建起来，就有什么订单的事实表、支付成功事实表、加工事实表，什么退单事实表、退款成功事实表等等这么多事实表，这是整体做处理的一些设计，那除此之外我们还做了一些其他方面的工作。
第一个比如说对于日志数据，原先 ODS 是在一个topic，我们需要把它分开分流，做一个分流的处理。这边我们使用的是测输出流的方案，将其分层启动页面曝光事件错误，这么几个 topic 存储在 d 到 d 层。同时在分流之后我们也会做一些简单处理，像新老用户修复等等。
另外就是我们，另外就是我们在构建事实表的过程中也遇到一些技术上的问题。首先第一个就是事实表刚才讲的几张，一个业务过程涉及到多张表，需要整合起来，这个地方就涉及到多流关联。那首先第一个我们会用到一个 regular join， regular join 的话倒还好，它的基本原理就是将来过的数据存在状态里，然后有数据来就去对方的流里面状态去匹配。但是我们这边有用到 left join，它会产生一个回撤流，因为两条流指不定谁的数据先来，它会先输出一个结果，之后再去通过回撤的方式去更新结果 join 结果。那对于这种回撤流的处理，我们由于 d 到 d 层还是选择Kafka，所以我们用的是一个 upset Kafka 连接器去写入。另外就是 regular join，它默认情况下这个 TTL 是没有去设置的，也就是说它默认不清理状态，那我们不能任由它状态无限膨胀，所以我们考虑设TTL，那在这一边我们主要用到 left doing 的是，主要是在业务上它们是同时产生的。比如说刚才讲的订单事实表，我们涉及到的订单表、详情表、状态表也好，我们当用户下了一笔订单之后，他们应该是同时生成的，或者说同时插入的一些相关业务数据，它们的事件时间应该是一样的，所以这边事件时间是一样，我们只需要考虑我们这个中间的一些网络延迟、处理延迟这些就可以了。
所以我们一开始 TTI 设置说设置几秒，但是最终考虑了一下还不行，那万一我上游的作业出现了故障，让我去处理的期间这个数据也是来不了的，处理恢复之后数据才会来，所以它可能还是有一个延迟，所以我们又考虑到故障处理时间预留出来，我们留了一个小时，所以差不多我们这 TTR 就是一个小时多一点。
另外一种类型的多流关联就是业务上面本身有时间差，也就是比如说我们的支付成功事实表这边，可能需要将一个订单表跟支付表去做一个关联。在这边我们系统是一笔用户下单之后最长有 30 分钟的支付时间，超过 30 分钟没支付订单会自动关闭，所以业务上他们最长可能有 30 分钟的时间差。
但我们本来也是想 run 跟 run join 直接写，但想更优雅一点，我们就使用的 interval join，因为 interval join 就不用，我们指定完下界和上界范围之后，它会自动管理状态，越过那个区间范围它是会去清理状态的，就比我们手动去管理更好一点。
所以像我们刚刚才说的支付成功事实表，我们用了 interval join，那这个上借下借的范围，我们就是考虑到那个业务时间差就可以了，当然还有那个流出故障的时间，但是其实故障时间不用考虑进去了，因为 interval tuning 它是基于事件时间来 Julian 的。
另外我们还使用到了一个叫 lookup 距离，我们对于name，我们对于字典表，像这种数据量小而且比较固定不怎么更新的表，我们是选择直接退化到事实表当中去，那我们直接用的是 LOOKUP doing 去做一个维度的补充， LOOKUP doing 其实就是现查外部系统，同时我们会开启它的异步功能，提高它的并发效率。同时也会开启它的 catch 缓存，减少对外部系统的访问，提高读取效率。这一些对于 circle 里面就是一些参数的配置。
但是这边我们发现了一个bug，如果不开异步，只开catch，可能会导致匹配到之上一条的结果，导致匹配结果是错的，那经过我们的测试发现它我们怀疑是个bug，但没去，我们是遇到了，但是我们发现就是只要开启异步，即使开了缓存但结果就不会出问题。
这是目前我觉得是个bug，要么两者都不开，那也没问题。我们实际用的时候就是异步也好，缓存也好，都是开启的，这是我们做的 lookup 作用的一些处理，主要是 d 到 d 层，再往后就是我们 DIM 层维表层的处理。维度表的设计我们首先考虑到它存储的位置，我们本来想的是存Kafka，但是一想不行，因为维度表我们需要做一个点查，还有随机查询，那点查可能就需要我要查询某一个指定的某一行，那 Kafka 这种不太适合消息队列，就天生不适合这种场景。更适合的是数据库，那数据库我们考虑的是直接用MySQL，但是 MySQL 是我们业务数据库，我们也考虑过再同步到一个另外的 MySQL 去做处理，但是最终我们组长就是就说这样，目前是没问题，但未来可能还要改架构，可能未来我们的数据量会增长。
其次 MySQL 对于这可能并发也好，连接数也好，可能也没有可能会顶不住，让未来再去做一些各种处理就很麻烦，我们还要费尽心思的可能要是什么去对它做索引，这那的，甚至数据量大还要做分库分表非常麻烦。那还不如直接用我们大数据生态圈的数据库，那我们考虑到分布式数据库又适合实时读写，点查性能又好，那就是Hbase，所以我们这个是长远考虑。
在之后我们这边的做法还有一个地方就是我们不希望在代码里面写死需要同步的这些维表，所以我们做了一个动态分流的方案，我们将想要同步的表字段等等，还有想同步的操作都维护成一张配置表，我们放在 MySQL 里面去手工维护配置表，然后我们这边尝试使用了一个 Flink CDC，因为在这里可以简化我们的链路，如果用 mess one 还得过一遍Kafka。另外这张表是手工维护的，就是即使出问题影响范围也没那么大，能够及时的去调整。也是尝试的用一下那 CVC 读取进来之后就读进来成一条流了，那我们再对齐做一个广播的操作，广播状态这个时候我们正常的数据流进来之后，就是读取 ODS 这个数据库。
topic 的这个流进来之后，从广播状态里面可以读取到配置信息，然后就将对应的配置信息，根据配置信息将对应的数据分别写出到 Hbase 不同的表，当然这里我们会作为一个判断，就是如果表不存在，我们会去创建创建，那在这里其实最早我们也踩过一些坑，就是这个 root key 的问题。
root key ROKEY 一开始我们考虑的直接，因为我们要查询维度，那基本上是通过它的维度表的主键，也就是 ID 值去做匹配的，那我们就想着那这样的话 Rocky 放 ID 也就是其组件是最合适的，但是这种 ID 都是自斟序列，很容易遇到一个数据热点的问题，我们避免这个问题，我们是做了一些处理，我们是对它做了一个加严。
我们会对原始的 ID 值有一个组件，做一个哈希，哈希完之后我们再磨上我们预分区的数量，我们预分区是分成了三个，然后磨上3，然后将这个结果补位，然后拼接在这个 ID 前面做一个加元处理，这样实现一个散裂的效果，然后从而避免一个数据热点问题。那回头我们去需要查询这个维度数据的时候，也是将它原始的组件池经过相同加盐逻辑处理，得到加盐后的 row key 再去查询 Hbase 就可以了。但是这么一些问题，另外我们也考虑了 Flink 写入 Hbase 会不会遇到一个数据一致性的问题，后面想一想，它这个 role key 本来就是具有一个唯一性的特点，即使我重复写入也没事，也不会出现说多行数据重复的情况，所以我们这边主要是依赖的是幂等性。
还有一个设计细节就是我们在这并没有对维度表做整合操作，因为在离线里面我们做的是信息模型，在这边选择雪花模型是为了避免再去做过多的 join 是可以，但考虑到实施场景中间 join 的表任何一张发生变化都要去更新结果，想要更新结果就得获取，就得跟其他表再次注意，那其他表的历史数据就得再那就得存在状态里，那这样的话就导致我们维度整合的这个理由，这个作业每条流的状态都不能经历，这样有点吃资源。所以我们直接那干脆选择就不去做整合了，就每张维度表，每张表还是分开的，各自更新各自的。
另外就是离线我们对用户表做了一个拉链的处理，在这边没做了，因为本来就是支持实时更新、实时同步，就一直保持全量最新的是那如果我们需要到，比如说用户的维度的历史状态，我们可以到我们离线数仓 DM 层里面去查询，但基本上不会用到，这个是 DIM 层我们做的一些处理。
再之后就是往后了，就是指标体系的事情了，我们还是一样将指标拆分成原子指标、派生指标还有衍生指标。原子指标就是原子的业务过程加上度量，加上聚合逻辑，比如说下单这个过程，度量就是次数，聚合逻辑就是count，就拆解成拆解到这个程度，那派生指标就是原子指标的基础上加上统计周期、统计力度、业务限定加原子指标，比如说最近一天或者当天，比如说力度可以是商品力度或者地区力度等等。
业务限定可以，比如说我分析某一个品类，比如说数码产品这种的业务限定，然后的一个下单汇总表，类似好下单次数，就这么一种指标是指标，另外就是衍生指标，衍生指标主要是派生指标做一个运算主要用的比较多的是求比率的时候，两个派生指标作为一个除法就可以了。相除，比如说退单率这种，留存率这种。
那我们将指标拆解完，形成体系之后，其实我们离线已经拆解差不多了，我们会摘几个出来，然后在首期我们实现的实施指标，在之后再提取他们公共的部分，然后去构建 DWS 层。那么在构建的时候我们这边没有去提取统计周期了，因为我们希望实时项目能够支持一个灵活的自助分析，他想怎么查就怎么查给到我们运营部门，因为后面运营部门我们那边给他们部署了一些 BI 工具，让他们能够很方便的及时怼到我们 DWS 层，作为一些快速的高效的机器查询，那所以我们就不会将数据高度聚合起来。
而既然如此，就我们就考虑到 DWS 层需要一个速度足够快，又能支撑大数据量这么一种 OLAP 数据库，所以当时我们对比了 CLICK house 跟Doris，克里克 house 可能是大家用的多，又速度又快， Doris 也是近几年火起来的国产的一个数据库，那我们简单的去测试调研了一下 CLICK house 的速度确实快，我们当时测了在1亿条数据， 180 多个字段的宽表作为一个单表分组聚合，也就是而且是单节点的 CLICK house 也就是毫秒级别的响应速度非常的快。但是克里克 house 我们感受了一下，运维部署特别麻烦，不管是新增节点还是对它做一些数据迁移等等，都得手动来，而且整个过程很容易出问题，这是我们的体会。确实强，用起来也麻烦。
那相比于Doris，我们觉得用起来特别舒服，特别爽，非常丝滑，因为它是基于 MySQL 客户端的，学习成本也低。它进程分为 FE 跟 BE 两类，那客户端是连接 FE 之后 FE 解析执行计划，然后分发给 BE 去执行，而我们在用的时候只需要用到 MySQL 客户端去连就可以了，而语法也基本跟 MySQL 保持一致，用起来非常的舒服和简单。
而且它运部署也好，运维也好，扩缩容也好，都很简单，主打的就是一个方便。那其实但是我们测试完之后，部分大部分场景下，它的速度比 clean house 略逊一筹，就是稍微慢那么一点，但是还行，大部分也能做到毫秒及秒级响应，你可以满足我们的需求，所以我们毫不犹豫选择了Doris。
最重要的一点还有一个就是关于 join 的能力， CLICK house 多表 join 能力非常弱，而 Doris join 能力比较强，它有各种 join 方式对吧？有什么 8 KEY 有广播join，什么 8 KEY join，甚至还有什么本地 8KEY join 等等。 join 适用不同场景，非常好还，而且还有各种什么索引，还有物化视图。
总而言之我们选了Doris，但是我们在使用过程中还是遇到过一些问题，我们当时最早用的是一系列的1点 1. 2 的，后面我们发现跑了一段时间会出现莫名其妙OM，后面我们以为是资源不够，也尝试过加资源，但是发现资源不管再怎么大，它一段时间之后总会出现OM，只不过出现的周期变长了而已。
所以我们就怀疑是不是我们的逻辑有问题，后面排查半天也发现不了问题，最终我们就求助去社区去找一些别的问题，还有社区里面去跟开发人员、官方人员去做一个交流，后面发现很多人都遇到就是他这个版本的内存管理有些问题，有些管理不好，所以我们当时的解决方案就是出现 OM 就快速的重启，只能这样子已经上线了。在后面他们说在 1. 2 系列里面说是对内存这一块做了优化和调整，所以我们后期是升了一下版本。
而且前面也讲到 Doris 部署运维简单，它的迁移升级也很简单，那升完之后确实好多了，至少我们就没遇到，但是其实我们在还关还注意到，嗯，重度使用 Doris 的还是会遇到OM，那包括最近像他们发布 2. 0 其实也说了还进一步去做了调整，所以等 2 系列来一个稳定板，我们可能还会继续去升级 Doris 版本。至少目前像我们这么用还好，没有太大问题。
好，这是关于这个组件选型考虑，还有遇到的一些小问题。那具体 DWS 层我们在处理的过程当中还有一些处理细节，处理逻辑上的细节。第一，刚才讲了我们这边不做高度聚合，做的是明细，那同时为了减轻写入的压力，我们这边是开了个窗，然后开窗做了一些预处理。其次，我们用那个连接器写入的时候可以设置批次，我们也是会长个批再去写入减轻写入的并发数，那 sorry 则更稳定一点。
那还有一个小细节，就是其实前面刚才讲了 DWD 层的事实表，有一些是通过 upset 去写入回测数据的，那我们去读取出来的时候，由于我们这边处理逻辑麻烦一点，没法直接写SQL，所以我们用 API 去实现的 DWS 层的作业。那在这个地方我们用的是普通的 Kafka 连接器，这样就会导致同一个 key 的它会有多条数据，也就是它的变化过程。
就撤回前跟撤回后这么多条数据都在，我们需要自己做一个去重处理，那我们这边做的是一个抵消的方案。抵消方案其实比如说 left join，它一开始可能左留先来，右流没来，那左边有直，右边是闹，后边又留的来了，那可能就是把原先这一条撤回，然后将新的结果插入。那这个时候我们只需要做一个抵消就可以了，就把三条合并成一条，最终结果就可以了，然后这条结果再去插入 Doris 就行了。
这边是我们做了一些处理，还有一个需要重点做的工作，就是构建 DWS 表的时候需要去补充维度信息，也就是涉及到维表join，那由于我们本身的方案采用的就是热存储加载，也就是将维度数据存到外部系统，那这里就是 Hbase 了，这个图这边写错了，是Hbase。
咳，那一开始我们去直接直连直查Hbase，后面遇到了一些反压的问题，后面当时那个高峰最高的 QBS 能达到那个几万的上万，上万，后面我们就尝试着做了一些调整跟优化。首先第一个我们对它做了一个异步的处理，因为 Flink 它本身有一个异步 IO 的功能，只需要我们使用它的异步客户端就可以了，那这边 Hbase r 系列的客户端是支持异步的，所以直接用就行。第二一个，为了减少对 Hbase 的访问，我们增加了一个旁路缓存使用的Redis，那在这我们就是优先去 Redis 查询缓存，如果没有才会去查询Hbase，查完之后再将数据缓存到 Redis 当中，大概就是这么一个处理的。那这样的话处理我们对 Hbase 的这个 QPS 就降低，效果特别明显，就下降了到100，甚至不到 100 平均，所以压力非常小。
那在这里还我们还得考虑一个问题，就是这个，因为刚才讲用到 Redis 做缓存，它首先有第一个问题，就是一个冷启动的问题，那我们首次启动诶，它里面是没缓存数据的，可能会造成一个类似缓存雪崩的问题。所以我们会提前做一个缓存预热，就是从离线数仓里面统计出最近 3 天热门的商品，而活跃用户等等，这一些主要是维度，然后提前将这一些维度表的数据插入到 Redis 当中去，避免这种类似雪崩的情况发生。
另外考虑到缓存穿透，我们就是对一些不存在都不存在的key，我们会给它缓也缓存下来，给个闹值，防止缓存击穿。我们是做了一个热门缓存的续约，我们做法比较简单，比如说每访问 100 次，我们就给他续一个小时，这么来实现。第二一个就是一直性问题，也就是怎么去保证 Hbase 跟 Redis 缓存是完全一致的？由于我们维度数据也是实时同步过来的，所以当我们监听到维度变更数据时，我们先删Redis，选择的是删除不是更新，为了保证一次性之后去更新 Hbase h base，更新完之后再去删一次Redis，防止说更新期间 Redis 又被缓存了老数据。也就是我们用的是一种延时双删的方案。
那这一块就是我们前面几层的处理，那最终 ADS 层我们这边没有去做一个持久化的动作，而是直接用 BI 工具直连我们的 Doris 现场。那这一边我们是我们这边开发了一个数据接口，我们 BI 工具是用了sugar， sugar 就对接这个数据接口去封装查询的一些SQL，等这些信息之后去现查 Doris 返回给接口，返回给BI，这样子做了一个。同时刚才也提到 DWS 层也是开放给我们的运营部门，可以随时做一些机器查询。另外我们对整个集群的监控，整个作业，整个项目作业监控用的是普罗米修斯进行一个监控指标采集，然后用的是 Grafana 做一个监控指标的图形化展示和告警，我们这边的告警会配置发送到我们的 OA 群里边和发邮件等等。
还有一个就是也是我们后期比较头痛的事情，也是一个痛点，就是整个实时数仓项目怎么保证结果的正确性，尽大可能的，或者说怎么保证实时数仓的数据质量，怎么对其做数据治理？在离线当中我们是做了一个数据治理的计分考量平台，我们是用这个平台实现的，但实施的话就比较不好做了，但是我们大概是按照这个思路来设计的，事前事中还是事前事中？事后事前也是定义好一些规范，比如说命名，比如说责任人，比如说一些提交参数的要求和规范提交模式等等，就是一些规范性的东西。嗯，还有就是要求，比如说还，我们还会具体要求到不同作业不同一致性，比如说你 check point，你进入一，要不要进入一次啊？你要不要 barrier 对齐，比如说写 kafka 要不要两阶段提交，下游要不要独立提交之类的等等，这一些写 Doris 要不要两阶段提交？还是唯一模型去做一个幂等等等之类的，我们得事前考虑好整个电动端到端的一致性。
适中其实就是我们整个集群作业在运行期间我们要做一些监控，监控刚才其实提到了就是普罗米修斯去监控。监控主要监控什么呢？它的运行状态是否成佛这个破影状态、 GVM 状态、反量状态、杠杆消费状态这些等等都能监控到。同时还有一些什么链路延迟，这些全都有，就是它的能不能正常运行，我们适中这边做的比较少，其实我们还想做的就是自定义监控指标，然后同时通过普罗米修斯采集过来，但是这个工作我们还没去具体去做。
自定义metric，就是指标， Flink 是支持的，可以去计数等等，这一些事后就是对一些事中监控发现的问题及时进行的一个复盘反思，然后有一些可以在事前就定义好，避免我们就会反馈到事前的这个流程当中，去进一步的增加我们规范的内容。整体来看其实就是还是一样按照事前、事中、事后形成一个闭环，逐步的完善，现在还属于初期，刚才讲的自定义 metric 我们还没怎么去做，这个整体就是我们整个项目的一些细节。那这个计算引擎我们使用的是Flink，我们主要部署模式是基于芽模式来的，因为芽模式可以动态对 PM 数量，动态申请、动态释放，对资源的管理比 standard 弄更加灵活。 standard alone 是启动集群之后资源长期占用不释放的那种，然后它的内存模型管理的也比较细致，有省略号，还有它提交流程运行原理省略号，还有我们曾经遇到过一些问题，从反例 2 开始讲起，然后讲资源的设置。讲啊，那个叫什么计算性能代码问题，对吧？怎么分析？怎么用火焰图去看？再讲讲曾经遇到的倾斜，再讲一类大状态的处理，也就是说全部引申出来。那这一块我就省略，你不一定讲好，等他来问。
